from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.svm import SVR, LinearSVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_log_error
Xtr, Xv, ytr, yv = train_test_split(train_x.values, y, test_size=0.2, random_state=1987)
rfr=RandomForestRegressor(n_estimators=60,max_depth=20,max_features='sqrt',)
rfr.fit(Xtr,ytr)
# y_p=rfr.predict(Xv)
# print('validation error:',mean_squared_log_error(yv,y_p))
# ytr_p=rfr.predict(Xtr)
# print('training error:',mean_squared_log_error(ytr,ytr_p))

# import xgboost as xgb
# from sklearn.model_selection import train_test_split
# Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)
# dtrain = xgb.DMatrix(Xtr, label=ytr)
# dvalid = xgb.DMatrix(Xv, label=yv)
# dtest = xgb.DMatrix(test[feature_names].values)
# watchlist = [(dtrain, 'train'), (dvalid, 'valid')]

# # Try different parameters! My favorite is random search :)
# xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,
#             'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,
#             'eval_metric': 'rmse', 'objective': 'reg:linear'}

# model = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,
#                   maximize=False, verbose_eval=10)

ytest = rfr.predict(test_x)
print('Test shape OK.') if test.shape[0] == ytest.shape[0] else print('Oops')
test['trip_duration'] = np.exp(ytest) - 1
test[['id', 'trip_duration']].to_csv('test_result.csv.gz', index=False, compression='gzip')
